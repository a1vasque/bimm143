---
title: "Class 8: Breast Cancer Mini-Project"
author: "Andres Vasquez (PID: A16278181)"
format: pdf
---

## About

Today's lab we will work with fine needle aspiration (FNA) of breast mass data from the University of Wisconsin

## Data Import

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names = 1)
head(wisc.df)
```

> How many patients are in this dataset?

```{r}
nrow(wisc.df)
```

> Q1. How many observations are in this dataset?

```{r}
wisc.data <- wisc.df[, -1]
ncol(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
diagnosis
```

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
inds <- grep("_mean", colnames(wisc.data))
length(inds)
```

```{r}
grep("_mean", colnames(wisc.data), value = T)
```

## Initial Analysis

Before analysis I want to take out the expert diagnosis column (aka the answer) from our dataset

```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

## Clustering

We can try `kmeans()` clustering first

```{r}
km <- kmeans(wisc.data, center = 2)
```

```{r}
table(km$cluster)
```

Cross table

```{r}
table(km$cluster, diagnosis)
```

```{r}
table(diagnosis)
```

Let's try `hclust()`. The key input required for `hclust()` is a distance matrix as produced by the `dist()` function.


```{r}
hc <- hclust(dist(wisc.data))
# I can make a tree life figure (dendrogram)
plot(hc)
```
```{r}
grps <- cutree(hc, k=2)
table(grps)
```

## PCA

Do we need to scale the data?

We can look at the sd of each column (original variable)

```{r}
round(apply(wisc.data,2, sd))
```
Yes, we need to scale. We will run`prcomp()` with `scale = T`

```{r}
wisc.pr <- prcomp(wisc.data, scale = T)
summary(wisc.pr)
```
Generate our main PCA plot (score plot, PC1 vs PC2, etc)

```{r}
library(ggplot2)
res <- as.data.frame(wisc.pr$x)
ggplot(res) + aes(PC1, PC2, col = diagnosis) + geom_point() + theme_bw() 
```

## Combining Methods

Clustering on PCA results

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Wardâ€™s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method = "ward.D2")
plot(hc)
```

To get my clustering result/membership vector I need to "cut" the tree with the `cutree()` function

> How many patients in each group?

```{r}
grps <- cutree(hc, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(res$PC1,res$PC2, col = grps)
```

## Prediction

We can use our PCA result (model) to do predictions, that is take new unseen data and project it onto our new PC vairables

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(res$PC1,res$PC2, col = grps)
points(npc[,1], npc[,2], col = "navy", pch = 16, cex = 2.5)
text(npc[,1], npc[,2], c(1,2), col="white")
```

# Summary

Principal Component Analysis (PCA) is a super useful method for analyzing large datasets. It works by finding new vairables (PCs) that capture the ost variance from the original variables in your datasets. 
- dimensionality reducition method